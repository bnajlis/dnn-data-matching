{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "This is a pure numpy implementation of word generation using an RNN\n",
    "\n",
    "![alt text](http://corochann.com/wp-content/uploads/2017/05/text_sequence_predict.png \"Logo Title Text 1\")\n",
    "\n",
    "We're going to have our network learn how to predict the next words in a given paragraph. This will require a recurrent architecture since the network will have to remember a sequence of characters. The order matters. 1000 iterations and we'll have pronouncable english. The longer the training time the better. You can feed it any text sequence (words, python, HTML, etc.)\n",
    "\n",
    "## What is a Recurrent Network?\n",
    "\n",
    "Feedforward networks are great for learning a pattern between a set of inputs and outputs.\n",
    "![alt text](https://www.researchgate.net/profile/Sajad_Jafari3/publication/275334508/figure/fig1/AS:294618722783233@1447253985297/Fig-1-Schematic-of-the-multilayer-feed-forward-neural-network-proposed-to-model-the.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://s-media-cache-ak0.pinimg.com/236x/10/29/a9/1029a9a0534a768b4c4c2b5341bdd003--city-year-math-patterns.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Hamza_Guellue/publication/223079746/figure/fig5/AS:305255788105731@1449790059371/Fig-5-Configuration-of-a-three-layered-feed-forward-neural-network.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "- temperature & location\n",
    "- height & weight\n",
    "- car speed and brand\n",
    "\n",
    "But what if the ordering of the data matters? \n",
    "\n",
    "![alt text](http://www.aboutcurrency.com/images/university/fxvideocourse/google_chart.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://news.mit.edu/sites/mit.edu.newsoffice/files/styles/news_article_image_top_slideshow/public/images/2016/vondrick-machine-learning-behavior-algorithm-mit-csail_0.jpg?itok=ruGmLJm2 \"Logo Title Text 1\")\n",
    "\n",
    "Alphabet, Lyrics of a song. These are stored using Conditional Memory. You can only access an element if you have access to the previous elements (like a linkedlist). \n",
    "\n",
    "Enter recurrent networks\n",
    "\n",
    "We feed the hidden state from the previous time step back into the the network at the next time step.\n",
    "\n",
    "![alt text](https://iamtrask.github.io/img/basic_recurrence_singleton.png \"Logo Title Text 1\")\n",
    "\n",
    "So instead of the data flow operation happening like this\n",
    "\n",
    "## input -> hidden -> output\n",
    "\n",
    "it happens like this\n",
    "\n",
    "## (input + prev_hidden) -> hidden -> output\n",
    "\n",
    "wait. Why not this?\n",
    "\n",
    "## (input + prev_input) -> hidden -> output\n",
    "\n",
    "Hidden recurrence learns what to remember whereas input recurrence is hard wired to just remember the immediately previous datapoint\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/ferret-rnn-151211092908/95/recurrent-neural-networks-part-1-theory-10-638.jpg?cb=1449826311 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.mathworks.com/help/examples/nnet/win64/RefLayRecNetExample_01.png \"Logo Title Text 1\")\n",
    "\n",
    "RNN Formula\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/0*TUFnE2arCrMrCvxH.png \"Logo Title Text 1\")\n",
    "\n",
    "It basically says the current hidden state h(t) is a function f of the previous hidden state h(t-1) and the current input x(t). The theta are the parameters of the function f. The network typically learns to use h(t) as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to t.\n",
    "\n",
    "Loss function\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/0*ZsEG2aWfgqtk9Qk5. \"Logo Title Text 1\")\n",
    "\n",
    "The total loss for a given sequence of x values paired with a sequence of y values would then be just the sum of the losses over all the time steps. For example, if L(t) is the negative log-likelihood\n",
    "of y (t) given x (1), . . . , x (t) , then sum them up you get the loss for the sequence \n",
    "\n",
    "\n",
    "## Our steps\n",
    "\n",
    "- Initialize weights randomly\n",
    "- Give the model a char pair (input char & target char. The target char is the char the network should guess, its the next char in our sequence)\n",
    "- Forward pass (We calculate the probability for every possible next char according to the state of the model, using the paramters)\n",
    "- Measure error (the distance between the previous probability and the target char)\n",
    "- We calculate gradients for each of our parameters to see their impact they have on the loss (backpropagation through time)\n",
    "- update all parameters in the direction via gradients that help to minimise the loss\n",
    "- Repeat! Until our loss is small AF\n",
    "\n",
    "## What are some use cases?\n",
    "\n",
    "- Time series prediction (weather forecasting, stock prices, traffic volume, etc. )\n",
    "- Sequential data generation (music, video, audio, etc.)\n",
    "\n",
    "## Other Examples\n",
    "\n",
    "-https://github.com/anujdutt9/RecurrentNeuralNetwork (binary addition)\n",
    "\n",
    "## What's next? \n",
    "\n",
    "1 LSTM Networks\n",
    "2 Bidirectional networks\n",
    "3 recursive networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code contains 4 parts\n",
    "* Load the trainning data\n",
    "  * encode char into vectors\n",
    "* Define the Recurrent Network\n",
    "* Define a loss function\n",
    "  * Forward pass\n",
    "  * Loss\n",
    "  * Backward pass\n",
    "* Define a function to create sentences from the model\n",
    "* Train the network\n",
    "  * Feed the network\n",
    "  * Calculate gradient and update the model parameters\n",
    "  * Output a text to see the progress of the training\n",
    " \n",
    "\n",
    "## Load the training data\n",
    "\n",
    "The network need a big txt file as an input.\n",
    "\n",
    "The content of the file will be used to train the network.\n",
    "\n",
    "I use Methamorphosis from Kafka (Public Domain). Because Kafka was one weird dude. I like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 16054 chars, 72 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('babel.txt', 'r', encoding='utf-8').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode/Decode char/vector\n",
    "\n",
    "Neural networks operate on vectors (a vector is an array of float)\n",
    "So we need a way to encode and decode a char as a vector.\n",
    "\n",
    "We'll count the number of unique chars (*vocab_size*). That will be the size of the vector. \n",
    "The vector contains only zero exept for the position of the char wherae the value is 1.\n",
    "\n",
    "#### So First let's calculate the *vocab_size*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E': 0, 'g': 1, '¡': 2, 'L': 3, 'r': 4, 'M': 5, 'S': 6, 'm': 7, 'j': 8, '¿': 9, 'Y': 10, '0': 11, 'é': 12, 'T': 13, '-': 14, 'b': 15, ' ': 16, 'l': 17, 'Q': 18, 'n': 19, ',': 20, 'É': 21, 't': 22, 'í': 23, 'ñ': 24, 'y': 25, '\\n': 26, 'z': 27, 'V': 28, 'P': 29, '«': 30, 's': 31, 'i': 32, 'u': 33, 'D': 34, 'ó': 35, '|': 36, 'B': 37, 'x': 38, '»': 39, 'p': 40, 'H': 41, 'C': 42, '1': 43, '2': 44, 'd': 45, 'A': 46, '©': 47, '?': 48, '.': 49, ')': 50, ':': 51, 'f': 52, '(': 53, 'v': 54, 'á': 55, '7': 56, 'N': 57, 'ú': 58, 'q': 59, 'U': 60, '!': 61, 'F': 62, 'h': 63, 'I': 64, 'a': 65, ';': 66, 'c': 67, 'O': 68, 'o': 69, 'R': 70, 'e': 71}\n",
      "{0: 'E', 1: 'g', 2: '¡', 3: 'L', 4: 'r', 5: 'M', 6: 'S', 7: 'm', 8: 'j', 9: '¿', 10: 'Y', 11: '0', 12: 'é', 13: 'T', 14: '-', 15: 'b', 16: ' ', 17: 'l', 18: 'Q', 19: 'n', 20: ',', 21: 'É', 22: 't', 23: 'í', 24: 'ñ', 25: 'y', 26: '\\n', 27: 'z', 28: 'V', 29: 'P', 30: '«', 31: 's', 32: 'i', 33: 'u', 34: 'D', 35: 'ó', 36: '|', 37: 'B', 38: 'x', 39: '»', 40: 'p', 41: 'H', 42: 'C', 43: '1', 44: '2', 45: 'd', 46: 'A', 47: '©', 48: '?', 49: '.', 50: ')', 51: ':', 52: 'f', 53: '(', 54: 'v', 55: 'á', 56: '7', 57: 'N', 58: 'ú', 59: 'q', 60: 'U', 61: '!', 62: 'F', 63: 'h', 64: 'I', 65: 'a', 66: ';', 67: 'c', 68: 'O', 69: 'o', 70: 'R', 71: 'e'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we create 2 dictionary to encode and decode a char to an int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finaly we create a vector from a char like this:\n",
    "The dictionary defined above allosw us to create a vector of size 61 instead of 256.  \n",
    "Here and exemple of the char 'a'  \n",
    "The vector contains only zeros, except at position char_to_ix['a'] where we put a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the network\n",
    "\n",
    "The neural network is made of 3 layers:\n",
    "* an input layer\n",
    "* an hidden layer\n",
    "* an output layer\n",
    "\n",
    "All layers are fully connected to the next one: each node of a layer are conected to all nodes of the next layer.\n",
    "The hidden layer is connected to the output and to itself: the values from an iteration are used for the next one.\n",
    "\n",
    "To centralise values that matter for the training (_hyper parameters_) we also define the _sequence lenght_ and the _learning rate_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are adjusted during the trainning.\n",
    "* _Wxh_ are parameters to connect a vector that contain one input to the hidden layer.\n",
    "* _Whh_ are parameters to connect the hidden layer to itself. This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
    "* _Why_ are parameters to connect the hidden layer to the output\n",
    "* _bh_ contains the hidden bias\n",
    "* _by_ contains the output bias\n",
    "\n",
    "You'll see in the next section how theses parameters are used to create a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "The __loss__ is a key concept in all neural networks training. \n",
    "It is a value that describe how good is our model.  \n",
    "The smaller the loss, the better our model is.  \n",
    "(A good model is a model where the predicted output is close to the training output)\n",
    "  \n",
    "During the training phase we want to minimize the loss.\n",
    "\n",
    "The loss function calculates the loss but also the gradients (see backward pass):\n",
    "* It perform a forward pass: calculate the next char given a char from the training set.\n",
    "* It calculate the loss by comparing the predicted char to the target char. (The target char is the input following char in the tranning set)\n",
    "* It calculate the backward pass to calculate the gradients \n",
    "\n",
    "This function take as input:\n",
    "* a list of input char\n",
    "* a list of target char\n",
    "* and the previous hidden state\n",
    "\n",
    "This function outputs:\n",
    "* the loss\n",
    "* the gradient for each parameters between layers\n",
    "* the last hidden state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "The forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next char given a char from the trainning set.\n",
    "\n",
    "xs[t] is the vector that encode the char at position t\n",
    "ps[t] is the probabilities for next char\n",
    "\n",
    "![alt text](https://deeplearning4j.org/img/recurrent_equation.png \"Logo Title Text 1\")\n",
    "\n",
    "```python\n",
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "```\n",
    "\n",
    "or is dirty pseudo code for each char\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh\n",
    "ys = hs*Why + by\n",
    "ps = normalized(ys)\n",
    "```\n",
    "\n",
    "### Backward pass\n",
    "\n",
    "The naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters.\n",
    "This is possible but would be time consuming.\n",
    "There is a technics to calculates all the gradients for all the parameters at once: the backdrop propagation.  \n",
    "Gradients are calculated in the oposite order of the forward pass, using simple technics.  \n",
    "\n",
    "#### goal is to calculate gradients for the forward formula:\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh  \n",
    "ys = hs*Why + by\n",
    "```\n",
    "\n",
    "The loss for one datapoint\n",
    "![alt text](http://i.imgur.com/LlIMvek.png \"Logo Title Text 1\")\n",
    "\n",
    "How should the computed scores inside f change tto decrease the loss? We'll need to derive a gradient to figure that out.\n",
    "\n",
    "Since all output units contribute to the error of each hidden unit we sum up all the gradients calculated at each time step in the sequence and use it to update the parameters. So our parameter gradients becomes :\n",
    "\n",
    "![alt text](http://i.imgur.com/Ig9WGqP.png \"Logo Title Text 1\")\n",
    "\n",
    "Our first gradient of our loss. We'll backpropagate this via chain rule\n",
    "\n",
    "![alt text](http://i.imgur.com/SOJcNLg.png \"Logo Title Text 1\")\n",
    "\n",
    "The chain rule is a method for finding the derivative of composite functions, or functions that are made by combining one or more functions.\n",
    "\n",
    "![alt text](http://i.imgur.com/3Z2Rfdi.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://mathpullzone-8231.kxcdn.com/wp-content/uploads/thechainrule-image3.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://i0.wp.com/www.mathbootcamps.com/wp-content/uploads/thechainrule-image1.jpg?w=900 \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  \"\"\"\n",
    "  #store our inputs, hidden states, outputs, and probability values\n",
    "  xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  #init loss as 0\n",
    "  loss = 0\n",
    "  # forward pass                                                                                                                                                                              \n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "    xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "  # backward pass: compute gradients going backwards    \n",
    "  #initalize vectors for gradient values for each set of weights \n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #output probabilities\n",
    "    dy = np.copy(ps[t])\n",
    "    #derive our first gradient\n",
    "    dy[targets[t]] -= 1 # backprop into y  \n",
    "    #compute output gradient -  output times hidden states transpose\n",
    "    #When we apply the transpose weight matrix,  \n",
    "    #we can think intuitively of this as moving the error backward\n",
    "    #through the network, giving us some sort of measure of the error \n",
    "    #at the output of the lth layer. \n",
    "    #output gradient\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    #derivative of output bias\n",
    "    dby += dy\n",
    "    #backpropagate!\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "    dbh += dhraw #derivative of hidden bias\n",
    "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "    dhnext = np.dot(Whh.T, dhraw) \n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sentence from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ñ2vVívxF,ú\n",
      "NDc«g|e:ñe-xfYréñRluThIñqfcEQ27oBdDéPul .|Péu;M|l)rUc©lM7óYdEíx|ñnF7u|2y.ñeh7í0qBp»«m)óÉQSñ!N»)oé«QñrCp»YÉHx7r,PbbHPMTovg(©pgxD?)\n",
      "VNt!ÉDCbBssOñ,MÉiQP.OIá«ótlOYT|xviL¿Oógv!lPmg;V|BáTBjn?caóy \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step   \n",
    "  n is how many characters to predict\n",
    "  \"\"\"\n",
    "  #create vector\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  #customize it for our seed char\n",
    "  x[seed_ix] = 1\n",
    "  #list to store generated chars\n",
    "  ixes = []\n",
    "  #for as many characters as we want to generate\n",
    "  for t in range(n):\n",
    "    #a hidden state at a given time step is a function \n",
    "    #of the input at the same time step modified by a weight matrix \n",
    "    #added to the hidden state of the previous time step \n",
    "    #multiplied by its own hidden state to hidden state matrix.\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    #compute output (unnormalised)\n",
    "    y = np.dot(Why, h) + by\n",
    "    ## probabilities for next chars\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    #pick one with the highest probability \n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    #create a vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for the predicted char\n",
    "    x[ix] = 1\n",
    "    #add it to the list\n",
    "    ixes.append(ix)\n",
    "\n",
    "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "  print('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training\n",
    "\n",
    "This last part of the code is the main trainning loop:\n",
    "* Feed the network with portion of the file. Size of chunk is *seq_lengh*\n",
    "* Use the loss function to:\n",
    "  * Do forward pass to calculate all parameters for the model for a given input/output pairs\n",
    "  * Do backward pass to calculate all gradiens\n",
    "* Print a sentence from a random seed using the parameters of the network\n",
    "* Update the model using the Adaptative Gradien technique Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feed the loss function with inputs and targets\n",
    "\n",
    "We create two array of char from the data file,\n",
    "the targets one is shifted compare to the inputs one.\n",
    "\n",
    "For each char in the input array, the target array give the char that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [3, 65, 16, 37, 32, 15, 17, 32, 69, 22, 71, 67, 65, 16, 45, 71, 16, 37, 65, 15, 71, 17, 26, 26, 16]\n",
      "targets [65, 16, 37, 32, 15, 17, 32, 69, 22, 71, 67, 65, 16, 45, 71, 16, 37, 65, 15, 71, 17, 26, 26, 16, 26]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print(\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print(\"targets\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad to update the parameters\n",
    "\n",
    "This is a type of gradient descent strategy\n",
    "\n",
    "![alt text](http://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/adagrad/adagrad2.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "\n",
    "step size = learning rate\n",
    "\n",
    "The easiest technics to update the parmeters of the model is this:\n",
    "\n",
    "```python\n",
    "param += dparam * step_size\n",
    "```\n",
    "Adagrad is a more efficient technique where the step_size are getting smaller during the training.\n",
    "\n",
    "It use a memory variable that grow over time:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "```\n",
    "and use it to calculate the step_size:\n",
    "```python\n",
    "step_size = 1./np.sqrt(mem + 1e-8)\n",
    "```\n",
    "In short:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update \n",
    "```\n",
    "\n",
    "### Smooth_loss\n",
    "\n",
    "Smooth_loss doesn't play any role in the training.\n",
    "It is just a low pass filtered version of the loss:\n",
    "```python\n",
    "smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "```\n",
    "\n",
    "It is a way to average the loss on over the last iterations to better track the progress\n",
    "\n",
    "\n",
    "### So finally\n",
    "Here the code of the main loop that does both trainning and generating text from times to times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 106.916646\n",
      "----\n",
      " :ñv)gPSqpebUbCdhB2uVMl1©T?L0CUi!(yN0ÉV\n",
      "tgSE 0gM»TvjI?fH» :CVIA\n",
      "\n",
      "Hj?o1»Olzú.TAcFfhúMRzAD(saFñ,í!7YdFFt2úyUt\n",
      ")ylÉx7mF1DrmeíY.ó©qzliMEfoCRyIáQIrQhxLyUbyúmV-OI)1BS.M2,fH;g?11Ib»zbRcéjHrQ.¿FDfpr¿Y-vc|vaOrx \n",
      "----\n",
      "iter 10000, loss: 47.157050\n",
      "----\n",
      " yecal de es cexiorís sicaro que la símaquel , en pargoo el es digcon este: nelas e noinodo tados mimecuro espuligión finentrado su la MCOvio banivimr toture y ros. Má un us mimote exlibre su vasporaña \n",
      "----\n",
      "iter 20000, loss: 42.532697\n",
      "----\n",
      " de la sicen halográs que mil esfcre. Yo, hos decos; menturar e ticabisca que de migorsirn a velegos: cajericlogues de hasBicienta un hinen ren númos de ualo (áfil, a fercin, en las de ingun dilos gano \n",
      "----\n",
      "iter 30000, loss: 39.801247\n",
      "----\n",
      "  caslióves. 7Cgél otro hoy lietíades,  le sin cada barirs a, col esta los ilublenzo, secerso en lictitar de lís despunger o, pre, comprepelo exista el veiltó nos des belien de de forfinte cheztiino mi \n",
      "----\n",
      "iter 40000, loss: 38.142656\n",
      "----\n",
      " letra de los libro no révacionisos pormoto: Ca en cavarsaba; el paribodan, que el sí eristre: Lada que suampre coro naté no amitilos. Unos de que de síproto intíado, ne lusterer el teciomen sechuno se \n",
      "----\n",
      "iter 50000, loss: 36.592416\n",
      "----\n",
      " las guazanzarmonte; ara, noresibros dal ormolos y l epos hembuén incelible el el varporio. Nonor, la hajtos del casa dintoguyque de Én infílo esíanos los soranos quí unquinivona lan un dimbrébras perm \n",
      "----\n",
      "iter 60000, loss: 35.559254\n",
      "----\n",
      " tarion comingiñas de los depuriatraton delos hay para telas.\n",
      "\n",
      " \n",
      "\n",
      "CB. sos iúmimilama elínuitrilo. di unútoríamen (on símbiés. Yo areren correnus; tieto de un biacibraciones de capiompres por hacpitate  \n",
      "----\n",
      "iter 70000, loss: 34.449803\n",
      "----\n",
      " ones patabre y de sormatrables, ez siscibres Madejo deimifere pendente, vidfvura | ya ventima cente inlada hombres hes del mensen er mintura. Ma sea ades unistade escreperente que el las singonos la j \n",
      "----\n",
      "iter 80000, loss: 33.595266\n",
      "----\n",
      "  exarracte en esca los mál idiado desesta ten endención ámi delera los todos conficio rentibasos hombres de le en el pesciones fos páginales hepésio secrosos, entoras no decesas de qus los pera resian \n",
      "----\n",
      "iter 90000, loss: 33.129365\n",
      "----\n",
      " ámó, percio, la la otrada: Lar con iddición los una trostro.\n",
      "Ca e; entur.\n",
      "\n",
      "\n",
      " El to hecános sinficas por singan la Bmbliote (ronúmentes hes caón estura que lo prorio se prodilas con que corrotrre escah \n",
      "----\n",
      "iter 100000, loss: 32.425531\n",
      "----\n",
      " n calimisiveras y de susínidasos redrició rengular sigue vibeleno de aún infinimersate soro esevarius. Haburan se los otro varáficos, liblo pero. Un está con que tinmi no haginas. Su ún hambión consab \n",
      "----\n",
      "iter 110000, loss: 31.976367\n",
      "----\n",
      " do libre y. Aceracre de os y ejempales erricionos y des intolúzaros porventer otren se existar abar prefira peroz litas, en aniente de cafalo sitre hable. Esésíicio inflidan que la dibuletían la aus a \n",
      "----\n",
      "iter 120000, loss: 31.495044\n",
      "----\n",
      " l ilciófecale, diezís aximprabló ontuyangen an Timzogresis, el combre que todo, por elterta several mule, Mus forimbiénes que toluma estrado se cacientirio milción osgala y hes cepcermite inconyego en \n",
      "----\n",
      "iter 130000, loss: 31.048192\n",
      "----\n",
      " os, perguros preverra las le que ese hallarios, histobabino y un denúmos. Que lay esíste es que la la que pire, dal conuso de inrentas haxionte. La aubres de sosperres, cado bibliotecarien o ban inabs \n",
      "----\n",
      "iter 140000, loss: 30.367803\n",
      "----\n",
      " inte; so dijegabló qulo zigiran es lubro tecunter natración relablal que me re hotras de las de anarener de leperira exagarión, que esfrestarinte soreverrue un zrmime litabliatral en ilibién la cada l \n",
      "----\n",
      "iter 150000, loss: 30.222561\n",
      "----\n",
      " erpores ilio vendo en compresteciones cona exer que el vistio es idaleíaba lo mino es Diotada sectigatura dedereros de atraco que Bibliates: el heyó, no incesto, ur cuyaso el susperadas o tones perve  \n",
      "----\n",
      "iter 160000, loss: 29.725395\n",
      "----\n",
      " bien cara la des pritiran paremeras. Yo auntilamo: pero ductiferso siesos des insenve sulero heda vivego, es cario, constidal siacibria - los libros aximimperarin e dí bise vormengos..\n",
      " Liote cuaceror \n",
      "----\n",
      "iter 170000, loss: 29.550686\n",
      "----\n",
      "  mos lubros abaprecióníasa iniren fostaje de un apara de lúpro exa palo - per bremital en er bepal qué otro sípohme alen seno arventoras sos voriesias hombien de en algunas zapéilicico. Nabiípecia al  \n",
      "----\n",
      "iter 180000, loss: 29.337844\n",
      "----\n",
      " sibién que Biblioteca no sisteleciaro mimparal guro hastro que la Biblioteca del nos antable, en uno bas fra titude se los hombres MCVgero, pisobrablos siotetra es pla prefiran die un hayerian hey gel \n",
      "----\n",
      "iter 190000, loss: 29.092826\n",
      "----\n",
      " urade algales por una var yioterarialesos sutras..\n",
      "\n",
      " \n",
      "\n",
      "C); ongunar saleriduran. Muanorria Biblioteca ascualimo:la que refor de un libros fandoros, pero que lostaclirno segura. Na e mara tural eno init \n",
      "----\n",
      "iter 200000, loss: 28.541222\n",
      "----\n",
      " ogiena, aráo invorstación de esccafinte un listo enta los que tecto desa tolagriban tisciotracua, que esidución simenéscao.\n",
      "\n",
      " \n",
      "\n",
      "El s, pesibras espocidos, el la tuglas.\n",
      "\n",
      " \n",
      "\n",
      "Hace lo devanciosos, de en a \n",
      "----\n",
      "iter 210000, loss: 28.509405\n",
      "----\n",
      "  legesido, ned hoy abres y semedeblo en er que na visáción de cada hexágonos le cara abicuar de un antación sina la Bábal sa) heguras extersenes.\n",
      "\n",
      " \n",
      "\n",
      "A la Dismeros; hustalil, rerretian en evo frenes V \n",
      "----\n",
      "iter 220000, loss: 28.383284\n",
      "----\n",
      " y ilibuulos dis la esed difirreles, me sospamio puede la destas. (y inempracios dituto. No de caumire, sescificiolesas enta o algrimado. Yo aorionterana en arganos que la empalogan al ún libros la cal \n",
      "----\n",
      "iter 230000, loss: 28.139624\n",
      "----\n",
      " de la vajumosar una sista zomugrabla Biblo Biverfudos, pero eno porseniran de anaquela y intura cuajquia recerariomegelos de codo  numunsa, costopurado que micaconte que se otra que no disutraz; algua \n",
      "----\n",
      "iter 240000, loss: 27.892930\n",
      "----\n",
      "  me na gunen siferso, no la distocan o. Atiníirique los mistición, cado que es idfinacióndas, fesubo. Inút escriferan de lotre elempangión rimintrosí: la hipente y de cuación coman de laguál es incito \n",
      "----\n",
      "iter 250000, loss: 27.669521\n",
      "----\n",
      " les, de analogosiorena a tada el vesda que ibilientable. Yo conúte, no extadaciones de los melecios, rentrirabía genar es cafires ungunos, inúma tove tasin, de venfinmente des vonimita Bienibras que s \n",
      "----\n",
      "iter 260000, loss: 27.327931\n",
      "----\n",
      " tar (lanodaba el venicio semaciones del combro fanena en alguna esccion: el mierpara la que el maldiñon, pero se lermate verivado: línstibe des inaque no seviompogira an que esgelecal es al cariunes u \n",
      "----\n",
      "iter 270000, loss: 27.238145\n",
      "----\n",
      " misa verdaso. Que la tado imundado poro muntaban factograríamen que telcio he Bibinablas por los sitriasezento, no eles sua no dijuitación, fuel dosper libro, críatición deses, y profagan y que en en  \n",
      "----\n",
      "iter 280000, loss: 27.229543\n",
      "----\n",
      " bio. Tombren su hoyo en que espr la ñer unvermor. Cus que el cerabe cablioteca del corce se compjema Biblioteca el ún la sietro mimétulor ala vendor hexágonos paresta, que los dimpales las atraciemper \n",
      "----\n",
      "iter 290000, loss: 27.014348\n",
      "----\n",
      " niveren el venerís; un acte dovor hombuén condente recia prestida en la tón la san motral se anique la Biblioteca hemotan nas libros de í. Esespersilo guasa enctriito profigedo.. Quresteran de susías: \n",
      "----\n",
      "iter 300000, loss: 26.770482\n",
      "----\n",
      " an obara dósperas, ma Mubliotecar s; en argión abirutirá de \n",
      "\n",
      "Imiduvinar de MCV eg pozo desore. La cios miluteleros  esperma que usa esa mplegelves, de no la vigonas, el tombron blgentigue una una auc \n",
      "----\n",
      "iter 310000, loss: 26.594904\n",
      "----\n",
      "  detá les incencia de infangularo pentuca y de calero.\n",
      "\n",
      " \n",
      "\n",
      "Cutirá: que ez «Tustidicagara distesco el yusto el idio mosteren y piomiven de la desente octena Biblable e perá conan el milta gonsa en algú \n",
      "----\n",
      "iter 320000, loss: 26.473281\n",
      "----\n",
      " isitras obro múntabresto, per qu dusto, que en itrodo diíme escmaban holdera de no lato: lo migsucene vezécificas de unfulto de universo, orstal que oda un hoy infinitudo, lo poro n un desparetía en u \n",
      "----\n",
      "iter 330000, loss: 26.372629\n",
      "----\n",
      " o en varos apagican per obiemprosple, porvele en suscia -pétima les infinir verventabintiendatro en listabletos: lotra en cos disto, in lemanas pon que el atrados en carámbosos remiti, letras se nadog \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 340000, loss: 26.342517\n",
      "----\n",
      " en aranos, focon los depetible, pero prespenures, el cada conúmense de las málal pusidocibién cada hexágono ilustra de las letra  en verio, calerían constrosco ques infinital la lab liomital ej intent \n",
      "----\n",
      "iter 350000, loss: 26.157316\n",
      "----\n",
      " s porsbida y encertas que de los lubla el merho y liso.\n",
      "\n",
      " \n",
      "\n",
      "E  fisáa (camo de MCV ei tosa contesto de un utasivación. (éNontes de  alimpto proble todos que cada prose des dilen de la defielecienen, in \n",
      "----\n",
      "iter 360000, loss: 26.073107\n",
      "----\n",
      "  espaloraban ya di esa guartuna ¿el ratudo esos anoquelturacablenes. Las uexagonar obsibro mel calluzares postilos y  divara: allagó y degio ma vestles de ojos en yas cada ligho. Si el hendo, n trduta \n",
      "----\n",
      "iter 370000, loss: 25.924052\n",
      "----\n",
      " uiotro singena puede desordaca provo siano prohimerio, esa del unvo de los libros namersepes pro pervida. Tobarario que uno hay incenciosa la hay pritación de los tuersa es atraciuto sa huye págimon d \n",
      "----\n",
      "iter 380000, loss: 25.766171\n",
      "----\n",
      " por en solo cuada universo veleventura de las de pero y se trdiguada un enomenel más que periabel tágras felmaza. Esares profigarin y de que adiminos que el depadad conomente valsto eser ompros que es \n",
      "----\n",
      "iter 390000, loss: 25.620188\n",
      "----\n",
      " bida ilpeticas, revotle de yis proviado: No ha figuian lo himembre ura secretras pero lablen de la la tuplifican rímbilacáate.\n",
      "\n",
      " \n",
      "\n",
      "Cobane o.\n",
      "\n",
      " \n",
      "\n",
      "Es ario, caajondados no profides: la ho homprol, el hoj \n",
      "----\n",
      "iter 400000, loss: 25.465318\n",
      "----\n",
      " o los fascitrento que los lique suemar, diegtodos los a de caásiminable. Veresí. Es arigan la pueden varial, an val ungen orcota mendo en costumcibinan los minaco de retosco. Tambre esplibres que año  \n",
      "----\n",
      "iter 410000, loss: 25.403455\n",
      "----\n",
      " asio es áfento, estras persetan en cerrimandedos proz nitras percia, percio, de lo prose que esos libtos peio homedasa de letra B que el Bablible.\n",
      "\n",
      " \n",
      "\n",
      "Oane, peno en valo desa tolas en otrabo ma prefie \n",
      "----\n",
      "iter 420000, loss: 25.309673\n",
      "----\n",
      "  casas rumbosos trado y parivanita; biblinte hombies la estidencias los carcuaron el uno nadevario memetido escara: La Bieno prose tociós pornudo», ual cormelituar esféricadoreron es cámroda un libros \n",
      "----\n",
      "iter 430000, loss: 25.263653\n",
      "----\n",
      " . Yo lo jumatar biblioteca, de can fienganos de esos la galerabi, el venecalentad inviles del buágensa, puede dibifibas laggilo, ben canfinado, el los hombres de renticación: nateríancen que mugnéncie \n",
      "----\n",
      "iter 440000, loss: 25.316558\n",
      "----\n",
      " ler ifreas, veveren de con ula compostas enco retras y hexágnáfil de los jelenías peryura, can yicrifilas que no valtagunara tiemista la men. Páelevermidas y plúmenesibros la jofino peibientas de ládo \n",
      "----\n",
      "iter 450000, loss: 25.117896\n",
      "----\n",
      " un pire, ferca de unas exivar morugrio que adía del jeteres perogías hexarodo y peros libros cada premule. No conde fervenerma que rustirictablónes larion la orvos, he fala, Lo siatocgana el combas pr \n",
      "----\n",
      "iter 460000, loss: 24.955651\n",
      "----\n",
      " ón las segiemonaba coma les diveribres; se cuateca ser otra dea tinue restular ib la tompiticin): La cuctro en la diversas persudo el místocias simenos libros. Uno, la la Biblioteca estura y vanfion d \n",
      "----\n",
      "iter 470000, loss: 25.012574\n",
      "----\n",
      "  fancionable siste inivocienerasos rágonsos que le Biblioteca en una compolte. Yo algiens los inbinte de piefro deser de los laz algunas vario de todos; todo niejos. Yo\n",
      "Tá) aredios repetodoso en exagr \n",
      "----\n",
      "iter 480000, loss: 24.688588\n",
      "----\n",
      " apónan taducricica, (orana sa Biblioteca ascuario no zdeles el las libra titres que la des facár los milto que esf eterio. Razonado-ligrio sintoguan que esn vercidia, que pesia que tomo diastologíapos \n",
      "----\n",
      "iter 490000, loss: 24.751726\n",
      "----\n",
      "  lo cualibros de que es incitratan y secreto, ma siemis cumanas. La vistulan e toda l: mujenfera y que la grimito poloz quel retenes, la vieros. Conma desespo pyá o honen VCe un ribliotrotinos los var \n",
      "----\n",
      "iter 500000, loss: 24.653502\n",
      "----\n",
      " sos descucerso: Para límersare des fetá la guéto de unaqueleran en itrmito de la lás de uni tedas; esdisa.\n",
      "\n",
      " \n",
      "\n",
      "Conte repugcá y aniquelenasio prey cuaran o de laturaran ese prefinaros mán celar veretra \n",
      "----\n",
      "iter 510000, loss: 24.670324\n",
      "----\n",
      " nces lan el miblicución cuarentid a los azadoroso, por ofigunay y que el esa pueda timerí ciempre dimeriente un vorvanas persichen, ha en eso pásta en eles inconto que tereice todo escibiente unos foc \n",
      "----\n",
      "iter 520000, loss: 24.864988\n",
      "----\n",
      " dumente mejévino, hegormoran con pliguanían sino variquej, arazo ngala escrempreban si usa hustas los libro Ao) exchámó, me inilicio | an qu e treita centa insongañinte son un perog\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Iambunfrerdi \n",
      "----\n",
      "iter 530000, loss: 24.601280\n",
      "----\n",
      " os pocívestra. Esosl casi tros y nfuran el ma la coma, masí... Esosibriotror dil ofigunaso, sien los acladoros, cadas lagque surpeazal pezbifentas, el vendorien vuajos, nodos los diembriblenía escára  \n",
      "----\n",
      "iter 540000, loss: 24.276214\n",
      "----\n",
      " nas, heje per una prefinito un librod absén uno reche ditecirmlita, respela ley infires los arden que esperos simpros que in espeluman dismotrodon zon el mistaban alguaranien grofos. Esos inctia, ha t \n",
      "----\n",
      "iter 550000, loss: 24.714349\n",
      "----\n",
      " se las lubro pero predo vasperos del clombocal que las lentictonte reces;r\n",
      ", y diito y lumoro, a la heyo, consuntas que tocta dalos ara hormabrón de las letra, el idomisuen de lCguen alaceregresal ha  \n",
      "----\n",
      "iter 560000, loss: 24.311608\n",
      "----\n",
      " n a deisares, el autrinaso en enomente un la depal gules renel tona de cuntiribles a preme, infinata de bud romájexintebrón. Ao niecidonamermales de esa juyasifre vonúmaba el ceppudicichas perde, argi \n",
      "----\n",
      "iter 570000, loss: 24.068429\n",
      "----\n",
      " giín, el milticas n teanestarían hepogos, el Bclaciones alcuno esta que camente el nfirmen se alden estería o del un cerficados y menguna el veno motor de caránició, ha bablirnus, el que emesción. Vas \n",
      "----\n",
      "iter 580000, loss: 24.294568\n",
      "----\n",
      " o vastición que el jupeda (sazYantos. Linvensiro 71 no una coma, nodo la tociés. ¿Lleros. ¿CóCa, cena ton en usoser abielquio natro. Es inverna sajabinir de que esos libros. Su nugtosta y exasiompelo  \n",
      "----\n",
      "iter 590000, loss: 24.185382\n",
      "----\n",
      " os la imiguras consatras. Que me letro. Nos obionto mezér, nodos curar e la fueriida se returos la mugno símrizor pal es a tol, incenta; arañoceriós es apacios, infímiy que mun donuncior. Cus envar de \n",
      "----\n",
      "iter 600000, loss: 24.145925\n",
      "----\n",
      "  Tambre casculo su la es dite intolegros. Que no rebpleguar otaguajen a jeicente un un demeridan procodica sin una tida sinquilas. Yo iniblir vastule pádal sa ves atratos promidos, peroblenos libron l \n",
      "----\n",
      "iter 610000, loss: 23.988838\n",
      "----\n",
      " e ladade es dioferara, codio prodo los. Azmo viames, iniquil basa) suajaza de cada mi máchos persacelía en es mil brámara, nodos que mi todos, y taluma exista; sal casculilte; cara mer vendar imigen d \n",
      "----\n",
      "iter 620000, loss: 23.915308\n",
      "----\n",
      " . Quien en probles Mue mis que es depiales din uncímo Bámoto hombién su Vuedadas hegro predua, en rata los infinido, la destrenien e perfacar (que una unfigiosos prdigalar: lo de finoreas de éntas fon \n",
      "----\n",
      "iter 630000, loss: 24.081579\n",
      "----\n",
      " a: lo que espagrulos májedia que los máledoras lo discirdado, los rmal, que tomprelegaros, con  los libroa en apécieros contetito.\n",
      "\n",
      " nemero  alguamo pero de la un atro diémplgida; hexágyáy comitema Bi \n",
      "----\n",
      "iter 640000, loss: 23.867427\n",
      "----\n",
      " ero puede arciom y los hombres) argoros que la Biblioteca es consenes; creto de na), ya su liburombil cuyo vermañenal dialesos brimunas; el en itragro problemordas que añoredacio de altrioler y ll vor \n",
      "----\n",
      "iter 650000, loss: 23.640161\n",
      "----\n",
      " ara impra -lesí, lo a sualeconosos pribra frete demiu tagiós. Mustrabinidó) e premete las piófina B esparan el hojalende un libros. Uno.\n",
      "\n",
      " \n",
      "\n",
      "A to; esta rembrasi predur guaran diren y sublisCg y lo gan \n",
      "----\n",
      "iter 660000, loss: 23.756873\n",
      "----\n",
      " ntungesos infióno posmaciones argéna Bibles die el cadable rumbralen el itros de es libro, no infiras. Esesprod de cada longen que verperio uro por una del comente y de falficas nombriotecias, alcimac \n",
      "----\n",
      "iter 670000, loss: 23.712247\n",
      "----\n",
      "  viajera de en verian de la Biblioteca es interdo, sermento infinienes, phedos la ipyostas yirsempoha - l que la Bibliotoca de un tentica y nibedas de con otigaler dialeraj estr diespre repato. A?)A;) \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 680000, loss: 23.615619\n",
      "----\n",
      " seas páginas de losuarios siglos procos, pero en el espagarso de sus alimpónfigas rebinato, esterios. De esa Bibliateca de un lis ula verdo y que en las libros númente promuras le de fienos las corume \n",
      "----\n",
      "iter 690000, loss: 23.665732\n",
      "----\n",
      " comparsicichamenesprgilar y de ruimete Comor vabse pros, ena otigras y bexTago: La Biblioteca y herra nalexino lo de un sioje esh rusticada quegíamen: que esas pitosangulirio, incospado y la ormejtado \n",
      "----\n",
      "iter 700000, loss: 23.337525\n",
      "----\n",
      "  el idio men sordo indurditraten a mbres de vabisa cacara barerabrón. Uno posibles cada estra milabal tadaso mbre su la Biblioteca es imar en al confirros escalecan de alad. Támorir de un se que la «m \n",
      "----\n",
      "iter 710000, loss: 23.464998\n",
      "----\n",
      " trazar es cagula (n usonte loso primera su esevaaman es discan sos hecáno mencre de mila coman el años. Sé lo diastodos y ser ar armeme, de un fieno diachifecioneres angon símente del cíar ma sojba re \n",
      "----\n",
      "iter 720000, loss: 23.307450\n",
      "----\n",
      "  de la laz no melicocos: la Biblioteca, digtan que los desucren)... La corrid lltramo».. Aombintuca es verdario ulbal salver que o\n",
      " \n",
      "\n",
      "La áme una ganes de orevera labricteciós, las viera los años, bara \n",
      "----\n",
      "iter 730000, loss: 23.313862\n",
      "----\n",
      "  curaro por que en borcirito, que reientable o es ingciemente tochemor, perogas. Hablir es aticias. De estras ensante rezí ofigues mondacuarsar en Biblioteca fervario; cacrámo la legrámiluban. Ancía q \n",
      "----\n",
      "iter 740000, loss: 23.426162\n",
      "----\n",
      " ímación de las canentraciales, que iúmine de una cuetados: el nadas biscibronos: el infinitidicas de idomora en tidos. Yo condes.\n",
      "\n",
      " \n",
      "\n",
      "Haciosa, bacimoría ¿e no pracere titatrar demida baccar existe rom \n",
      "----\n",
      "iter 750000, loss: 23.230651\n",
      "----\n",
      " n diesel ada yible de lo justicado de Aás guazondedo en rambre (que esclibros, codo, beríasa de esplas deeleren que el sin «la Biblioteca al vendo de la Biblioteca freren cada condas porsto, pero en r \n",
      "----\n",
      "iter 760000, loss: 23.356244\n",
      "----\n",
      "  octablato de un ay de ides on ispécido: Nomóntormes... Lo unose coba los de comar viojes es falecarion exestosos cunararia y de calería, que opecos prejer que tada la coriemploba los aporíaras altida \n",
      "----\n",
      "iter 770000, loss: 23.326271\n",
      "----\n",
      "  que no que restiricu condible existivo.. Químpal consubión cuafunár; un onto purción bla veneran y al que la conendes, con dijedo, o lo prificos la miz ondesplama que me «se ele oaloctosista. Si uso  \n",
      "----\n",
      "iter 780000, loss: 23.143305\n",
      "----\n",
      " cas pormenesos.. Nobrendes todina enteran en ilba de las mujrad aunúras. Haba tena ana comafe todo de inorposos cada mis las tules pedo los viompre; ma o, incerá de dienos iducustía s piomirencios pre \n",
      "----\n",
      "iter 790000, loss: 23.147217\n",
      "----\n",
      " es sos otra dojemprestabse mindirio no en er apconas sós di lenua, de uno un alinticaces quo no en el enteraninan que en revo digros pribi, pluy) rudo, posibliamente del en dulag\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " La atra patien \n",
      "----\n",
      "iter 800000, loss: 22.978904\n",
      "----\n",
      " peregriaros prabun dermizabana de cada letras, la predirumer hexágnta; hexágono prepurficas nama.\n",
      "\n",
      " \n",
      "\n",
      "AA compremerma un escamen (cablió, pero (canabre rabirecho.:.\n",
      "\n",
      "\n",
      " Aombres fungionio es tos librézab \n",
      "----\n",
      "iter 810000, loss: 22.991104\n",
      "----\n",
      " s que estico de escribable. La de unfumerible ja bibliotecarierma, ejes universo, que extaciores min verios s peruó, pero las alo: en aldencuaje prohibibiban mi la dectilos notiglojes dedías. Elegía e \n",
      "----\n",
      "iter 820000, loss: 23.014954\n",
      "----\n",
      "  ancenarar unfaciónen sipocio al gúfioso se rentacricicas; pado que má fol infedobla, lo en ungambriotecay comanto rímbies de verseciones de otra sipécida abigníitis, rer ora sualera hemide salida cot \n",
      "----\n",
      "iter 830000, loss: 23.014686\n",
      "----\n",
      "  los aparaba la exas) se esa belesí: libros, per etración de sas posía el inimitaciles págonabl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Ana fela del muría casa lo condelemineciónes aleras problemetro mandicidicuban. Yo repital revante  \n",
      "----\n",
      "iter 840000, loss: 22.971752\n",
      "----\n",
      " los intonada letro esa deleníates se len ilumpalcifir de ojós: lo que la forma del conBido en al que es sel esda de MCV ez atacio na Bd y la lol primersa cabillia estrezgo die el mulúmer upritada (oru \n",
      "----\n",
      "iter 850000, loss: 22.793987\n",
      "----\n",
      " iuso de en depa grios fatiliconido cresinitan camerio rexágono su libo, la verdad. En uvoneverá. De labuan en emocal evalindel er casusción) a jendencios los aféraran de la sol olbe a se problema y de \n",
      "----\n",
      "iter 860000, loss: 23.098251\n",
      "----\n",
      " reciorulas lo primerado nos enútilos. Vus m; adienó y que es libliotigusio en la todagea símbien injerpó las dijásifirsicifagan escconaban a del pensta, conablerna y de que las letra o minel sos histo \n",
      "----\n",
      "iter 870000, loss: 22.815020\n",
      "----\n",
      " y que en algunas, revero ganiribe estrietro. Vios cuóndcima hay anoredamarsos, perda un uno promunguron des puede diátido, pero en CLa sia de tospe de un evoricio, es compritudabenciran y le dobrosper \n",
      "----\n",
      "iter 880000, loss: 23.015546\n",
      "----\n",
      "  iospitencian (nobos mol es lado, milemas imimimeresice se vigifica posibión, cabinfiemente parmulirma son cada constirmabrestas perfecesía fagión cúmor.gnas, elcexientlación es sinciones perces abiem \n",
      "----\n",
      "iter 890000, loss: 22.827261\n",
      "----\n",
      " enó lo movéitira can lablizte m; tacionos úpredi delos auterimo se libri; s péaminabrescito. Altura; que las suy hexáj; que tado y un trefecaloras y dos hepogés. Púeio prezar en la Biblioteca y desple \n",
      "----\n",
      "iter 900000, loss: 22.832751\n",
      "----\n",
      " ncencios ispaciones altal dias los libros y de rustodo y hecorme, línes de cansibrado. Esos parueno pasevario severo invules carante es el confirme temagel esa en uno intable en catáarar ique lás fino \n",
      "----\n",
      "iter 910000, loss: 23.056304\n",
      "----\n",
      " ogurecal que los dimpro desirccicas y misterios mengun e predul, yo éb ébsivios diamerí,, pero dos páginatos man quo no estigion la hecta dalTodo y de espelez, aunúlenidas,.\n",
      "\n",
      " \n",
      "\n",
      "D y tonies profisa val \n",
      "----\n",
      "iter 920000, loss: 22.519653\n",
      "----\n",
      " n hieliras, y lumalos linjegulares. Quiz o el cltuloren que la esta los siente una paza conteca; escrifron alguna a se hucrido, discen las leredos no posta y de canasccieria, consubra postulas. SBiCum \n",
      "----\n",
      "iter 930000, loss: 23.020780\n",
      "----\n",
      " sta. En esturicado en listables, de carguna pan que el cuctraz a se refinas; cariumples de en que napasa escresio de ombun en estabarana 71 coher, el heyores y de cecuro e promproba gonaécrictas de lí \n",
      "----\n",
      "iter 940000, loss: 22.668844\n",
      "----\n",
      " cerablen el disogros. Mualgo, que deser, el hexágono secáren sienos congales. Pmunales hon Han did parazamines, escales de sugosia la en el divoras; cerpelman mosporicifro que lo rasos nter azarara se \n",
      "----\n",
      "iter 950000, loss: 22.607546\n",
      "----\n",
      " ino se disilto sinte con, comejen ma todagcado es infune de or de uno premero del Oviomeres y me fiel ovargoses de las galerías el vendo fatiagretesio encien y que de; distición. Mien hexágono seanacr \n",
      "----\n",
      "iter 960000, loss: 22.744599\n",
      "----\n",
      " do y que se reianico, per du libues letras. Poniodos para falumonte sa mul los hombrés fendo en que esen es infermo y suendo: La cuctos, elcertas» (coyobrébión quier divario si ene es pero son ilógici \n",
      "----\n",
      "iter 970000, loss: 22.731555\n",
      "----\n",
      "  la espete; se mbribles se un tero: Para la otrabíración genes; du sublis, una siater azarme, cacluzar que piez, en artigan unútigen los ariungudal. La se umi. Mubales, farios sulació pendos projo dea \n",
      "----\n",
      "iter 980000, loss: 22.621338\n",
      "----\n",
      " o pero sinquicrefro nugros probibles de mila porognió, mpreciales de un jotraban el lor una libros, que deilte, note.\n",
      "\n",
      " \n",
      "\n",
      "A líme desmantadas (que inaquel de lun iúfinitor dilsente Biblioteca.\n",
      "\n",
      " \n",
      "\n",
      "Esio \n",
      "----\n",
      "iter 990000, loss: 22.381070\n",
      "----\n",
      " dimuto fatia dospero mansas, hesó y sigles codo nos libros puedes en tecto;ía ilumposta uncembinavela y no exfutaran la tunor de es infinir, que esos nodes sibecarón. Du laruma infata dembinaficario e \n",
      "----\n",
      "iter 1000000, loss: 22.508157\n",
      "----\n",
      " efiemay de la otrchoso reprosifaran sal métádar (yque no mbreble (Liocesar.. A mico, inceriovar ornuy que es llizo, méperan secciosos bios. Paría - \n",
      "Coros, cesfación en exía fansultos que inzata suCom \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*1000:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "  if p+seq_length+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data                                                                                                                                                             \n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "  if n % 10000 == 0:\n",
    "    print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "  p += seq_length # move data pointer                                                                                                                                                         \n",
    "  n += 1 # iteration counter    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
